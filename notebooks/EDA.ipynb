{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes Dataset - EDA\n",
    "\n",
    "In this notebook, an initial Exploratory Data Analysis (EDA) is performed, followed by data cleaning and the plotting of insights related to the data contained in the dataset `diabetes_unclean.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening and First look\n",
    "In this section, the necessary libraries are imported, and a preliminary analysis of the dataset is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../datasets/diabetes_unclean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset features\n",
    "In the dataset we can access the following feature:\n",
    "\n",
    "- **Gender**: The gender of the individual (e.g., Male, Female).\n",
    "- **AGE**: The age of the individual in years.\n",
    "- **Urea**: The level of urea in the blood, indicating kidney function. \n",
    "- **Cr**: Creatinine level in the blood, used to assess kidney function.\n",
    "- **HbA1c**: Hemoglobin A1c percentage, a measure of average blood sugar levels.\n",
    "- **Chol**: Total cholesterol level in the blood, measured in mmol/L, indicating lipid profile.\n",
    "- **TG**: Triglycerides level in the blood, measured in mmol/L, another component of the lipid profile.\n",
    "- **HDL**: High-Density Lipoprotein cholesterol, often referred to as \"good cholesterol,\" measured in mmol/L.\n",
    "- **LDL**: Low-Density Lipoprotein cholesterol, often referred to as \"bad cholesterol,\" measured in mmol/L.\n",
    "- **VLDL**: Very Low-Density Lipoprotein cholesterol, another type of \"bad cholesterol,\" measured in mmol/L.\n",
    "- **BMI**: Body Mass Index, a measure of body fat based on height and weight.\n",
    "- **CLASS**: The target variable indicating the presence or absence of diabetes (e.g., diabetic or non-diabetic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing\n",
    "In this section, data cleaning is performed by analyzing entries that are outside the acceptable range or are potential outliers. Additionally, preprocessing and encoding of non-numeric or categorical features are carried out.\n",
    "\n",
    "\n",
    "List of task to be performed in this section:\n",
    "- Backup the original dataset\n",
    "- Check for Null values\n",
    "- Conversion to numerical values or one-hot encoding of the 'object' or 'string' types\n",
    "- Check for outliers and cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_backup = dataset.copy()\n",
    "ds_backup.to_csv('../datasets/diabetes_unclean_backup.csv', index=False) # This saves a backup of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of missing values in each column\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null values \n",
    "Since the total entries of the dataset are 1009 and the Null values are less than the 2% of the dataset it is possible to remove them (no fill required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the rows with missing values and applying the changes to the dataset\n",
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns to be converted or encoded:\n",
    "- Gender -> simple encoding [0 = 'M', 1 = 'F']\n",
    "- Class -> simple encoding [0 = non-positive, 1 = positive]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Gender'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are inconsistencies in the gender column we will perform a 'normalization' of the data and a conversion to numeric value \n",
    "```\n",
    "[0: Male, 1: Female]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repairing inconsistent values and Encoding of the 'Gender' column\n",
    "dataset['Gender'] = dataset['Gender'].str.upper().map({'M': 0, 'F': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['CLASS'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are several unique values with inconsistent syntax, since the possible value of the column `CLASS` are 'Diabetic' or 'Not Diabetic' we will encode it with the following syntax:\n",
    "```\n",
    "[Positive to diabetes: 1]\n",
    "[Negative to diabetes: 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding of the 'CLASS' column\n",
    "# 1. remove unwanted spaces\n",
    "dataset['CLASS'] = dataset['CLASS'].str.strip()\n",
    "# 2. raise all values to upper case\n",
    "dataset['CLASS'] = dataset['CLASS'].str.upper()\n",
    "# 3. make the values consistent (e.g. P means positive as well as Y, instead, N means negative or simply No)\n",
    "dataset['CLASS'] = dataset['CLASS'].replace({'P': 'Y'})\n",
    "\n",
    "# check:\n",
    "# dataset['CLASS'].unique()\n",
    "\n",
    "# 4. Encoding of the 'CLASS' column\n",
    "dataset['CLASS'] = dataset['CLASS'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused columns\n",
    "Some columns are not relevant to predict diabetes. For example, columns like `ID` and `No_Pation` do not provide meaningful information for prediction and can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unused columns\n",
    "# backup of the cleaned and encoded dataset\n",
    "ds_backup = dataset.copy()\n",
    "# removing unused columns\n",
    "dataset.drop(columns=['ID', 'No_Pation'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each class\n",
    "class_counts = dataset['CLASS'].value_counts()\n",
    "\n",
    "# Calculate the percentage\n",
    "class_percentages = (class_counts / len(dataset)) * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"Diabetes (CLASS = 1):\")\n",
    "print(f\"Count: {class_counts[1]}, Percentage: {class_percentages[1]:.2f}%\")\n",
    "print(\"\\nNo Diabetes (CLASS = 0):\")\n",
    "print(f\"Count: {class_counts[0]}, Percentage: {class_percentages[0]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplot of the features to get an overall view of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataset, diag_kind='kde', hue='CLASS')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "In this section, we identify outliers to ensure the dataset is consistent, enabling accurate analysis and reliable predictions using future models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = dataset.drop(columns=['Gender', 'AGE']).describe() \n",
    "# storing the stats in a dataframe (excluding gender and age columns)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filtering columns with a standard deviation greater than half the mean and store it in a list\n",
    "(retrieved from the just made 'stats' dataframe) \n",
    "'''\n",
    "\n",
    "\n",
    "# Boolean mask to filter columns: \n",
    "high_std_mask = stats.loc['std'] > (stats.loc['mean'] / 2)\n",
    "# Apply the mask and create a list of 'suspicious' features:\n",
    "features_high_std = stats.columns[high_std_mask] \n",
    "suspicious_features = list(features_high_std)\n",
    "print(suspicious_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of the features as subplots horizontally\n",
    "\n",
    "plt.figure(figsize=(15, 20))\n",
    "\n",
    "# Loop through each feature and create a subplot: \n",
    "# i is the index of the feature in the dataset, feature is the name of the feature\n",
    "for i, feature in enumerate(dataset.columns, 1):\n",
    "    plt.subplot(len(dataset.columns), 1, i)         # Arrange subplots vertically\n",
    "    sns.boxplot(data=dataset[feature], orient='h')  # Set orientation to horizontal\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection Analysis\n",
    "\n",
    "## Features Requiring Special Attention\n",
    "\n",
    "As shown in the preliminary analysis, the features **'Urea', 'Cr', 'TG', 'HDL',** and **'VLDL'** show high values of standard deviation ($\\sigma$). Specifically, these columns have standard deviation values higher than half their respective means:\n",
    "\n",
    "$$\\text{Attention required when:}\\quad \\sigma > \\frac{\\text{mean}}{2}$$\n",
    "\n",
    "This indicates potential outliers that warrant further investigation.\n",
    "\n",
    "## Physiological Limits for Outlier Detection\n",
    "\n",
    "Physiological ranges and limits:\n",
    "\n",
    "| Feature | Lower Limit | Upper Limit | Justification |\n",
    "|---------|-------------|-------------|---------------|\n",
    "| **Urea** | 1 mmol/L | 25 mmol/L | Values outside this range are extremely rare in living patients and likely represent measurement errors |\n",
    "| **Cr (Creatinine)** | 10 µmol/L | 400 µmol/L | Values above 400 µmol/L may indicate severe renal failure but could also be data entry errors; values below 10 µmol/L are biologically implausible |\n",
    "| **TG (Triglycerides)** | 0.1 mmol/L | 10 mmol/L | While normal range is <1.7 mmol/L, values up to 10 mmol/L can occur in severe hypertriglyceridemia or **diabetic** patients |\n",
    "| **HDL** | 0.3 mmol/L | 5 mmol/L | Values above 5 mmol/L are highly improbable and likely due to laboratory errors or data entry mistakes |\n",
    "| **VLDL** | 0.05 mmol/L | ? mmol/L | Value derived from a medical equation that considers the levels of TG and cholesterol in the blood. Further investigation needed, possible syntethic values not needed for this project. |\n",
    "\n",
    "\n",
    "  *! Diclaimer*: The reference values provided are not intended for medical purposes and were obtained through online research. They are not guaranteed to be reliable or representative of the sample in question, as this is not the primary objective of the project.\n",
    "  Sources: https://www.scymed.com, https://www.my-personaltrainer.it/salute/conversione-colesterolo.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box-plot of the suspicious feature:\n",
    "In the box-plot above, it's evident that features such as 'Urea', 'Cr', 'TG', 'HDL', and 'VLDL' exhibit substantial variance with an asymmetric skew towards the upper extremes. However, we can't straightforwardly discard data above the third quartile since these patients might be suffering from diabetes-related pathologies or severe dysfunctions, information that could be needed in the prediction model and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting and storing in lists the upper and lower limits for each suspicious feature:\n",
    "#                   Urea, Cr,  TG, HDL, VLDL\n",
    "temp_limits_upper = [25,  400, 10,  5,   40 ]\n",
    "temp_limits_lower = [1,   10,  0.1, 0.3, 0.05]\n",
    "\n",
    "# converting it to a dataframe for better readability:\n",
    "suspicious_features_boundaries = pd.DataFrame({'Feature': suspicious_features, 'Upper Limit': temp_limits_upper, 'Lower Limit': temp_limits_lower})\n",
    "\n",
    "suspicious_features_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identification of the values that exceed the biological plausibility thresholds and removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plotting the same boxplots for the suspicious features adding the upper and lower limits\n",
    "for graphical reference:\n",
    "'''\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "data = dataset[features_high_std]\n",
    "\n",
    "# Loop through each feature and create a subplot\n",
    "for i, feature in enumerate(features_high_std, 1):\n",
    "    plt.subplot(len(features_high_std), 1, i)  # Arrange subplots vertically\n",
    "    sns.boxplot(data=data[feature], orient='h')  # Set orientation to horizontal\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    \n",
    "    # Red line for the upper threshold\n",
    "    plt.axvline(x=temp_limits_upper[i - 1], color='red', linestyle='-', label='Threshold')\n",
    "    \n",
    "    # Ged line for the lower threshold\n",
    "    if temp_limits_lower[i - 1] > 0:\n",
    "           plt.axvline(x=temp_limits_lower[i - 1], color='green', linestyle='-', label='Threshold') \n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the number of data-points above the thresholds for each abnormal variance column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add two new columns to the suspicious_features_boundaries DataFrame\n",
    "suspicious_features_boundaries['Above upper limits'] = [\n",
    "    data[feature][data[feature] > suspicious_features_boundaries.loc[i, 'Upper Limit']].count()\n",
    "    for i, feature in enumerate(suspicious_features_boundaries['Feature'])\n",
    "]\n",
    "\n",
    "suspicious_features_boundaries['Below lower limits'] = [\n",
    "    data[feature][data[feature] < suspicious_features_boundaries.loc[i, 'Lower Limit']].count()\n",
    "    for i, feature in enumerate(suspicious_features_boundaries['Feature'])\n",
    "]\n",
    "\n",
    "# Display the DataFrame\n",
    "suspicious_features_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing or replacing outliers\n",
    "\n",
    "For now, we replace the outliers with NaN values. This approach allows us to retain the flexibility to either remove these entries or handle them differently in subsequent steps, depending on the requirements of the analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing with NaN\n",
    "\n",
    "for index, row in suspicious_features_boundaries.iterrows():\n",
    "    feature = row['Feature']\n",
    "    upper_limit = row['Upper Limit']\n",
    "    lower_limit = row['Lower Limit']\n",
    "    dataset.loc[(dataset[feature] > upper_limit) | (dataset[feature] < lower_limit), feature] = np.nan\n",
    "\n",
    "# counting the number of missing values in each column\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLDL and TG\n",
    "Since VLDL is a value typically synthetic, we investigate its distribution in relation to TG. The scatter plot below highlights their relationship, showing a positive correlation between the two variables.\n",
    "\n",
    "VLDL can also be measured directly in some cases, thus the relation with the TG value can be non-linear in some cases.\n",
    "\n",
    "Formulas for computing VLDL starting from TG:\n",
    "$$VLDL = \\frac{\\text{TG}}{5} \\quad \\text{(mg/dL)}$$ \n",
    "$$VLDL = \\frac{\\text{TG}}{2.2} \\quad \\text{(mmol/L)}$$\n",
    "\n",
    "#### Conversion rate between different units of measurement:\n",
    "- (TG):\n",
    "\n",
    "$$\\text{mmol/L} = \\frac{\\text{mg/dL}}{88.5}$$\n",
    "$$\\text{mg/dL} = \\text{mmol/L} \\times 88.5$$\n",
    "\n",
    "- Cholesterol (LDL, HDL, VLDL, Totale):\n",
    "\n",
    "$$\\text{mmol/L} = \\frac{\\text{mg/dL}}{38.67}$$\n",
    "$$\\text{mg/dL} = \\text{mmol/L} \\times 38.67$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for TG vs VLDL with color based on threshold\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create a standalone pandas Series to classify points based on the threshold\n",
    "VLDL_thresh_status = pd.Series(np.where(dataset['VLDL'] > 4, 'above', 'below'), index=dataset.index)\n",
    "\n",
    "\n",
    "\n",
    "# Scatter plot with color coding\n",
    "sns.scatterplot(data=dataset, x='TG', y='VLDL', hue=VLDL_thresh_status, alpha=0.5)\n",
    "\n",
    "plt.title('Scatter Plot: TG vs VLDL')\n",
    "plt.xlabel('TG')\n",
    "plt.ylabel('VLDL')\n",
    "plt.axhline(y=4, color='red', linestyle='--', label='Hypothesis Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset for VLDL values under 4\n",
    "filtered_dataset = dataset[dataset['VLDL'] < 4]\n",
    "\n",
    "# Scatter plot for TG vs VLDL with VLDL < 4\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(data=filtered_dataset, x='TG', y='VLDL', alpha=0.5, hue=VLDL_thresh_status)\n",
    "plt.title('Scatter Plot: TG vs VLDL (VLDL < 4)')\n",
    "plt.xlabel('TG')\n",
    "plt.ylabel('VLDL')\n",
    "\n",
    "# Plot the line y = x / 2.2\n",
    "x_vals = np.linspace(filtered_dataset['TG'].min(), filtered_dataset['TG'].max(), 100)\n",
    "y_vals = x_vals / 2.2\n",
    "plt.plot(x_vals, y_vals, color='red', label='VLDL = TG / 2.2')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset for VLDL values over 4\n",
    "filtered_dataset = dataset[dataset['VLDL'] >= 4]\n",
    "\n",
    "# Scatter plot for TG vs VLDL with VLDL >= 4\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(data=filtered_dataset, x='TG', y='VLDL', alpha=0.5, hue=VLDL_thresh_status)\n",
    "plt.title('Scatter Plot: TG vs VLDL (VLDL >= 4)')\n",
    "plt.xlabel('TG')\n",
    "plt.ylabel('VLDL')\n",
    "plt.grid(True)\n",
    "\n",
    "x_vals = np.linspace(filtered_dataset['TG'].min(), filtered_dataset['TG'].max(), 100)\n",
    "y_vals = x_vals * 38.67 / 5.5\n",
    "plt.plot(x_vals, y_vals, color='red', label='VLDL = TG * 38.67 / 5.5')\n",
    "plt.legend()\n",
    "\n",
    "# VLDL = TG * 38.67 / 5.5\n",
    "# TG = VLDL * 5.5 / 38.67\n",
    "# converted_VLDL = (VLDL * 5.5 / 38.67) / 2.2\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are two groups of VLDL values, as seen in the graphs above. \n",
    "\n",
    "- One group consists of values less than 4, following a pseudo-linear distribution composed of synthetic values derived from the standard formula ($VLDL = TG / 2.2$) with the unit of measurement: mmol/L. \n",
    "- The other group exhibits an apparently incorrect trend. Further analysis has demonstrated that the second group represents measurements in a different unit of measurement (mg/dL). \n",
    "\n",
    "To homogenize the data, it will be necessary to convert the VLDL measurements back to TG and re-compute the VLDL value as in the other group:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert VLDL values greater than 4 using the new formula\n",
    "dataset.loc[dataset['VLDL'] > 4, 'VLDL'] = (dataset['VLDL'] * 5.5 / 38.67) / 2.2\n",
    "\n",
    "# Scatter plot for TG vs VLDL after conversion\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(data=dataset, x='TG', y='VLDL', alpha=0.5)\n",
    "plt.title('Scatter Plot: TG vs VLDL (After Conversion)')\n",
    "plt.xlabel('TG')\n",
    "plt.ylabel('VLDL')\n",
    "\n",
    "# Plot the line y = x / 2.2\n",
    "x_vals = np.linspace(dataset['TG'].min(), dataset['TG'].max(), 100)\n",
    "y_vals = x_vals / 2.2\n",
    "plt.plot(x_vals, y_vals, color='red', label='VLDL = TG / 2.2')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows with one or more NaN values\n",
    "rows_with_nan = dataset.isnull().any(axis=1).sum()\n",
    "\n",
    "# Calculate the percentage of rows with NaN values\n",
    "percentage_with_nan = (rows_with_nan / len(dataset)) * 100\n",
    "\n",
    "print(f\"Number of rows with NaN values: {rows_with_nan}\")\n",
    "print(f\"Percentage of rows with NaN values: {percentage_with_nan:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('../datasets/diabetes_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
